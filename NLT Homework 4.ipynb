{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "Next word predictor using a bigram language model\n",
    " \n",
    "> Input: A word\n",
    ">\n",
    "> Output : List of words that can follow the input word, and their corresponding probabilities \n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1. Build a bigram LM using the following two steps:\n",
    "\n",
    "    A. Use nltk to compile all the unique bigrams from the corpus you used for the previous assignment.  \n",
    "\n",
    "    B. Compute probability of each bigram using MLE (count(w1 w2)/count(w1)) \n",
    "\n",
    "2. Predict next word using the following steps:\n",
    "\n",
    "    A. Get an input word from user, inpW.\n",
    "\n",
    "    B. Use the bigram LM built in step 1 to find all the bigrams where the input word, inpW, is w1.  Display all possible next words from these bigrams and their corresponding probabilities.  (Sort in descending order on probabilities)\n",
    "\n",
    "-----\n",
    "\n",
    "Example:\n",
    "\n",
    "Toy Corpus: \"I will go to California to meet my friend\"\n",
    "\n",
    "Output of Step 1:\n",
    "\n",
    "Bigram model: \n",
    "P(will | I) = 1,\n",
    "\n",
    "P(go | will) = 1,\n",
    "\n",
    "P(to | go) = 1,\n",
    "\n",
    "P(California | to) = 0.5, \n",
    "\n",
    "P(to | California) = 1, \n",
    "\n",
    "P(meet | to) = 0.5, \n",
    "\n",
    "P(my | meet) = 1, \n",
    "\n",
    "P(friend | my) = 1\n",
    "\n",
    "\n",
    "User Input string: \"to\"\n",
    "\n",
    "\n",
    "Output: \"Possible next words: California: 0.5, meet: 0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing A Corpus\n",
    "As instructed, we are to reuse the same corpus as the previous assignment. In my case, I am using Barack Obama's 2013 inaugural speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/leavism/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "nltk.download('inaugural')\n",
    "obama_speech = inaugural.words(fileids='2013-Obama.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use NLTK's utilities to compile all unique bigrams from the corpus. NLTK's bigram function takes a sequence as an iterator to return the bigram. The example in NLTK's documentation shows the usage as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipping\n",
    "list(bigrams([1,2,3,4,5]))\n",
    "# > [(1, 2), (2, 3), (3, 4), (4, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this, it's important to note that we need to feed in the tokenized corpus, which is already provided to us. However, NLTK also stored punctuations as separate words. I kept the punctuation in the last assignment, but I will be removing them in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "obama_speech = [word for word in obama_speech if re.match(r'^\\w+$', word)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
